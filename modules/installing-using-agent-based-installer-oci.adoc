// Module included in the following assemblies:
//
// * installing/installing_oci/installing-oci-agent-based-installer.adoc [Using the Agent-based Installer to install a cluster on OCI]

:_mod-docs-content-type: CONCEPT
[id="installing-using-agent-based-installer-oci_{context}"]
= Using the Agent-based Installer to install a cluster on OCI

You can harness all the advantages of the Agent-based installer to install an {product-title} cluster on {oci-first}. The Agent-based installation comprises a bootable ISO that contains the Assisted discovery agent and the Assisted Service. Both of this components are required to perform the cluster installation, but the latter component runs on only one of the hosts.

:FeatureName: Using the Agent-based Installer to install an {product-title} cluster on OCI
include::snippets/technology-preview.adoc[]

The Agent-based installer provides the ease of use of the Assisted Installation service, but with the capability to install a cluster in either a connected or disconnected environment. Additionally, you can use the Agent-based Installer to generate or accept Zero Touch Provisioning (ZTP) custom resources.

[NOTE]
====
Consider selecting a nonvolatile memory express (NVMe) drive or a solid-state drive (SSD) for your boot disk, because these drives offer low latency and high throughput capabilities for your boot disk. 
====

Before you install {product-title} on {oci}, you must complete some required steps for creating an {oci} environment on your virtual machine (VM) shape or bare-metal shape. By completing these steps, you can install {product-title} and deploy a cluster on infrastructure that supports a wide range of cloud options and strong security policies.

.Prerequisites
* You have prior knowledge of {oci} components. See link:https://docs.oracle.com/en-us/iaas/Content/GSG/Concepts/concepts.htm[Learn About Oracle Cloud Basics] in the Oracle documentation.  
* You reviewed details about the xref:../../architecture/architecture-installation.html#installation-overview_architecture-installation[{product-title} installation and update processes].
//* You completed the [Request Access to Red Hat OpenShift on {oci} in Developer Preview](https://docs.google.com/forms/d/e/1FAIpQLSdQHmw0BUWjQxPKC5_G7lk2_o9Pcnn2ON84Al6s2Ir254gv6Q/viewform) form. 
* You read the documentation on xref:../../installing/installing-preparing.html#installing-preparing-selecting-cluster-type[Selecting a cluster installation method and preparing it for users].
* You have read the xref:../../installing_with_agent_based_installer/preparing-to-install-with-agent-based-installer.html[Preparing to install with the Agent-based Installer] documentation.
* You downloaded the xref:../../installing/installing_with_agent_based_installer/installing-with-agent-based-installer.html#installing-ocp-agent-retrieve_installing-with-agent-based-installer[Agent-Based Installer] and the command-line interface (CLI) from Red Hat’s Hybrid Cloud Console.
* For a disconnected environment, you created a container image registry, such as Red Hat Quay. See xref:../../installing/disconnected_install/installing-mirroring-creating-registry.html[Creating a mirror registry for Red Hat OpenShift].
* Your organization signed up for an Oracle account and Identity Domain. This step is required so that you can access an administrator account, which is the initial cloud-identity and access management (IAM) user for your organization. See link:https://docs.oracle.com/en-us/iaas/Content/Identity/Concepts/overview.htm#ariaid-title4[The administrators group and policy] section in the Oracle documentation. 
* You have logged into the {product-title} with administrator privileges. 
* You have logged into your organization’s {oci} account with administrator privileges.

.Procedure

. Create a compartment and ensure you defined your Oracle® Cloud ID (OCID) in the compartment. A compartment is a component where you can organize and isolate your cloud resources. After you create a compartment, Oracle automatically assigns an OCID to your organization’s account. An administrator can access all compartments tagged to your organization’s OCI account. See [Creating compartments](https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managingcompartments.htm#ariaid-title5) in the Oracle documentation. 

2. Create a virtual cloud network (VCN). A compute instance, load balancer, and other resources need this network infrastructure to connect with each other over an internet connection. To establish an on-premise network you must manually create subnets, gateways, routing rules, and security policies. See [Creating a VCN](https://docs.oracle.com/en-us/iaas/Content/Network/Tasks/create_vcn.htm) in the Oracle documentation. 

3. Create a network security group (NSG) in your VCN. You can use the NSG to establish advanced security rules for your network. You must locate the NSG in your compartment, so that certain groups can access network resources. See [Creating an NSG](https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/create-nsg.htm) in the Oracle documentation. 

4. Create a dynamic group that hosts compute instances. After you create the dynamic group, you can then create a policy statement that defines rules for your cluster environment. This statement sets the precedent  for each compute instance to join your OpenShift Container Platform cluster as a self-managed node. See [Creating a dynamic group and a policy for self-managed nodes](https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdynamicgrouppolicyforselfmanagednodes.htm). 

5. Create a policy statement. See [Managing policies](https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managingpolicies.htm) in the Oracle documentation. You must create a policy so that your administrator can grant access to your groups, users, or resources that operate in your network.

6. Create a load balancer, so that you can provide automated traffic distribution on your VCN. See [Creating a load balancer](https://docs.oracle.com/en-us/iaas/Content/Balance/Tasks/managingloadbalancer_topic-Creating_Load_Balancers.htm) in the Oracle documentation. 

7. Add a record to a Domain Name System (DNS), so that you Oracle’s edge-network can maintain your domain’s DNS queries. See [Adding a record to a DNS zone](https://docs.oracle.com/en-us/iaas/Content/DNS/Tasks/record-add.htm) in the Oracle documentation. 

    **Note:** Each DNS record must share the same public and private ID of the load balancer. Configure the `agent-config.yaml` configuration file to meet your organization’s requirements. 

    **Example `agent-config.yaml` configuration file that sets values for an IPv4 formatted network.**
    ~~~
    apiVersion: v1alpha1
    metadata:
      name: <cluster_name> <1>
      namespace: <cluster_namespace> <2>
    rendezvousIP:<ip_address_from_CIDR> <3>
    bootArtifactsBaseURL:<server_URL> <4>
    # …
    ~~~
    <sup>1</sup> The cluster name that you specified in your DNS record. 
   <sup>2</sup> The name of your cluster on OpenShift Container Platform. 
   <sup>3</sup> If you are using IPv4 as the network IP address format, ensure that you set the *rendezvousIP* parameter to an IPv4 address that the VCN’s Classless Inter-Domain Routing (CIDR) allocates on your network. 
   <sup>4</sup> The URL of the server where you want to upload the `rootfs` image. 

9. Configure the `install-config.yaml` configuration file to meet the needs of your organization. 

    **Example `install-config.yaml` configuration file that demonstrates setting an external platform**
    ~~~
    # install-config.yaml
    apiVersion: v1
    metadata:
      name: <cluster_name> <1>
    baseDomain: <base_domain> <2>
    networking:
      clusterNetwork:
      - cidr: 10.128.0.0/14
        hostPrefix: 23
      network type: OVNKubernetes
      machineNetwork:
      - cidr: <ip_address_from_cidr> <3>
      serviceNetwork: 
      - 172.30.0.0/16
    compute:
      - architecture: amd64 <4>
      hyperthreading: Enabled
      name: worker
      replicas: 0
    controlPlane:
      architecture: amd64 <4>
      hyperthreading: Enabled
      name: master
      replicas: 3
    platform:
      external:
        platformName: oci <5>
        cloudControllerManager: External
    sshKey: <public_ssh_key> <6>
    pullSecret: '<pull_secret>' <7>
    # ...
    ~~~
   <sup>1</sup> The cluster name that you specified in your DNS record. 
   <sup>2</sup> The base domain of your cloud provider. 
   <sup>3</sup> The IP address from the VCN that the CIDR allocates to resources and components that operate on your network.
   <sup>4</sup> Depending on your infrastructure, you can select either `x86_64`, or `amd64`.
   <sup>5</sup> Set `OCI` as the external platform, so that OpenShift Container Platform can integrate with OCI. 
   <sup>6</sup> Specify your SSH public key.
   <sup>7</sup> The pull secret that you need for authenticate purposes when downloading container images for OpenShift Container Platform components and services, such as Quay.io. See [Install OpenShift Container Platform 4](https://console.redhat.com/openshift/install/pull-secret) from the Red Hat Hybrid Cloud Console. 

10. Create a directory on your local system named `openshift`. 
    
    **Important:** Do not move the `install-config.yaml` and `agent-config.yaml` configuration files to the `openshift` directory.

11. From the [`oracle-quickstart / oci-openshift`](https://github.com/oracle-quickstart/oci-openshift) GitHub web page, select the **<> Code** button and click **Download ZIP**. Save the archive file to your `openshift` directory,  so that all the CCM and CSI manifests exist in the same directory. The downloaded archive file includes files for creating cluster resources and custom manifests. Click the following link to access the custom manifest files:

    - [CCM and CSI custom manifests](https://github.com/oracle-quickstart/oci-openshift/tree/main/custom_manifests).
    
    The CCM manifest are required for deploying Oracle’s CCM during cluster installation so that OpenShift Container Platform can connect with the external OCI platform. The CSI custom manifests are required for deploying Oracle’s CSI driver during cluster installation so that OpenShift Container Platform can claim required objects from OCI.

    **Important:** You must modify the secret `oci-cloud-controller-manager` defined in the [oci-ccm.yml](https://github.com/oracle-quickstart/oci-openshift/blob/main/custom_manifests/manifests/oci-ccm.yml) configuration file to match your organization's region, compartment OCID, VCN OCID, and the subnet OCID from the load balancer.

12. Use the Agent-based Installer to generate a minimal ISO image, which excludes the `rootfs` image, by entering the following command in your OpenShift Container Platform CLI. You can use this image later in the process to boot all your cluster’s nodes. 

    ~~~
    $ ./openshift-install agent create image --log-level debug
    ~~~

   The previous command also completes the following actions:

   - Creates a subdirectory, `./<installation_directory>/auth directory:`, and places `kubeadmin-password` and `kubeconfig` files in the subdirectory.  
   - Creates a `rendezvousIP` file based on the IP address that you specified in the `agent-config.yaml` configuration file. 
   - Optional: Any modifications you made to `agent-config.yaml` and `install-config.yaml` configuration files get imported to the Zero Touch Provisioning (ZTP) custom resources. 

     **Important:** The Agent-based Installer uses Red Hat Enterprise Linux CoreOS (RHCOS). The `rootfs` image, which is mentioned in a subsequent listed item,  is required for booting, recovering, and repairing your operating system. 

13. Apply one of the following two updates to your `agent-config.yaml` configuration file:

   - For a disconnected network:  After you run the command to generate a minimal ISO Image, the Agent-based installer saves the `rootfs` image into the `./<installation_directory>/boot-artifacts` directory on your local system. Upload `rootfs` to the location stated in the `bootArtifactsBaseURL` parameter in the `agent-config.yaml` configuration file. For example, if the URL states *http://192.168.122.20*, you would upload the generated `rootfs` image to this location, so that the installer can access the image from `http://192.168.122.20/agent.x86_64-rootfs.img`.  After the installer boots the minimal ISO for the external platform, the Agent-based Installer downloads the `rootfs` image from the `http://192.168.122.20/agent.x86_64-rootfs.img` location into the system memory. 

   **Note:** The Agent-based Installer also adds the value of the `bootArtifactsBaseURL` to the minimal ISO Image’s configuration, so that when the Operator boots a cluster’s node, the installer downloads the `rootfs` image into system memory.

   - For a connected network: You do not need to specify the `bootArtifactsBaseURL` parameter in the `agent-config.yaml` configuration file, because the Agent-based Installer, by default, reads the a `rootfs` URL location from *https://rhcos.mirror.openshift.com*. After the installer boots the minimal ISO for the external platform, the installer then downloads the rootfs file into your system’s memory from the default RHCOS URL.

   **Important:**  Consider that the full ISO image, which is in excess of `1` GB, includes the `rootfs` image and the image is considerably larger than the minimal ISO Image, which is typical less than `150` MB.

14. Upload the Agent ISO image to Oracle’s default Object Storage bucket and then import the Agent ISO image as a custom image to this bucket. You must then configure the custom image to boot in Unified Extensible Firmware Interface (UEFI) mode.  See [Creating a custom image](https://docs.oracle.com/en-us/iaas/secure-desktops/create-custom-image.htm) and [Using the Console](https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringimagecapabilities.htm#ariaid-title5) in Oracle’s documentation.

    For example, from **Compute > Custom images**, import the Agent ISO image to the bucket, and enter values in the following fields:

    - Name: *oci-cluster*
    - Bucket: Select the bucket where you uploaded the discovery ISO image
    - Object name: Select the name of the discovery ISO
    - Image type: **QCOW2**

    After the image imports, go to the **Edit image capabilities** setting and ensure  that `UEFI_64`  is selected for the **Firmware** field. 

15. Create a compute instance from the supplied base image for your preferred cluster topology, such as a single-node OpenShift (SNO) cluster, a highly-availability cluster that contains a minimum of 3 control plane instances and two compute instances, or a compact three-node cluster that contains a minimum of three control plane instances. See [Creating an instance](https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/launchinginstance.htm#top) (Oracle documentation).

    **Important:** Before you create the compute instance, check that you have enough memory and disk resources for your cluster. See [Recommended resources for topologies](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/installing/installing-an-on-premise-cluster-with-the-agent-based-installer#recommended-resources-for-topologies).  Additionally, ensure that at least one compute instance has the same IP address as the address stated under `rendezvousIP` in the `agent-config.yaml` file .

    The following example lists important settings for an instance named *oci-cluster-master*. 

    - Go to **Image and shape section >  Image >  My images** and then select your custom image.  
    - Go to  **Image and shape section > Shape menu** and then select at least 4 CPUs and 16 GB of memory.
    - In **Primary VNIC IP addresses > Primary network**, select a VCN, such as *oci-cluster-vcn*. From the **Subnet** section, select your subnet, such as *ici-cluster-private-subnet*. For public IPV4 subnets, ensure that you select the **Do not assign a public IPv4 address** checkbox.
    - Click **Show advanced options**. Select the **Use network security groups** to control traffic checkbox, and then set your NSG, such as *oci-cluster-controlplane-nsg*.
    - From the **Boot volume** section, select the **Specify a custom boot volume size** checkbox. Enter a value that is at least `100` GB for the boot volume size. Allocate the number of VPUs for your organization needs, such as `100`. 

### Verification

After you deploy the compute instance on a self-managed node in your OpenShift Container Platform cluster, you can monitor the cluster’s status by choosing one of the following options:

- From the OpenShift Container Platform CLI, enter the following command:
    ~~~
    $ ./openshift-install agent wait-for install-complete --log-level 
    debug
    ~~~

    Check the status of the `rendezvous` host node that runs the bootstrap node.  After the host reboots, the host forms part of the cluster.

- Use the `kubeconfig` API to check the status of various OpenShift Container Platform components. For the  `KUBECONFIG` environment variable, set the relative path of the cluster’s `kubeconfig` configuration file:
    ~~~
    $  export KUBECONFIG=~/auth/kubeconfig
    ~~~

    Check the status of each of the cluster’s self-managed nodes. CCM applies a label to each node to designate the node as running in a cluster on OCI. 

    ~~~
    $ oc get nodes -A
    ~~~

    **Output example:**
    ~~~
    NAME                                   STATUS ROLES                 AGE VERSION 
    main-0.private.agenttest.oraclevcn.com Ready  control-plane, master 7m  v1.27.4+6eeca63
    main-1.private.agenttest.oraclevcn.com Ready  control-plane, master 15m v1.27.4+d7fa83f
    main-2.private.agenttest.oraclevcn.com Ready  control-plane, master 15m v1.27.4+d7fa83f
    ~~~

    Check the status of each of the cluster’s Operators, with the CCM Operator status being a good indicator that your cluster is running. 
    ~~~
    $ oc get co
    ~~~

    **Truncated output example:**
    ~~~
    NAME           VERSION     AVAILABLE  PROGRESSING    DEGRADED   SINCE   MESSAGE
    authentication 4.15.0-0    True       False          False      6m18s
    baremetal      4.15.0-0    True       False          False      2m42s
    network        4.15.0-0    True       True           False      5m58s  Progressing: …
    …
    ~~~
